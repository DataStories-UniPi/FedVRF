{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import contextily as ctx\n",
    "\n",
    "import numpy as np\n",
    "import datetime \n",
    "\n",
    "import sys, os\n",
    "import tqdm\n",
    "import shapely\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import st_toolkit as stt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_points = pd.read_csv('./data/piraeus-dataset/datasetpr_split_trajectories_sets_shuffle.csv', parse_dates=['timestamp'])\n",
    "actual_points = gpd.GeoDataFrame(actual_points, geometry=gpd.points_from_xy(actual_points['WGS84lon'], actual_points['WGS84lat']), crs=4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load VRF Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models as ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vrf_piraeus_pth_sn_cml   = torch.load('./data/pth/lstm_1_350_fc_150_share_all_window_1024_stride_1024_crs_3857___batchsize_1__piraeus_dataset.pth', map_location=torch.device('cpu'))\n",
    "vrf_piraeus_pth_sd_cml   = torch.load('./data/pth/lstm_1_350_fc_150_share_all_window_1024_stride_1024_crs_3857___batchsize_1__share_all.pth', map_location=torch.device('cpu'))\n",
    "fedvrf_piraeus_pth       = torch.load('./data/pth/fl/lstm_1_350_fc_150_window_1024_stride_1024_crs_3857___.flwr_global_epoch170.pth', map_location=torch.device('cpu'))\n",
    "perfl_fedvrf_piraeus_pth = torch.load('./data/pth/perfl/lstm_1_350_fc_150_window_1024_stride_1024_crs_3857___.flwr_global_epoch170.pth', map_location=torch.device('cpu'))\n",
    "\n",
    "# Instantiate SN-CML VRF Model\n",
    "sn_cml_model = ml.VesselRouteForecasting(\n",
    "    hidden_size=350, fc_layers=[150,], \n",
    "    scale=dict(\n",
    "        mu=torch.tensor(vrf_piraeus_pth_sn_cml['scaler'].mean_[:2]), \n",
    "        sigma=torch.tensor(vrf_piraeus_pth_sn_cml['scaler'].scale_[:2])\n",
    "    )\n",
    ")\n",
    "sn_cml_model.load_state_dict(vrf_piraeus_pth_sn_cml['model_state_dict'])\n",
    "sn_cml_model.eval()\n",
    "\n",
    "# Instantiate SD-CML VRF Model\n",
    "sd_cml_model = ml.VesselRouteForecasting(\n",
    "    hidden_size=350, fc_layers=[150,], \n",
    "    scale=dict(\n",
    "        mu=torch.tensor(vrf_piraeus_pth_sd_cml['scaler'].mean_[:2]), \n",
    "        sigma=torch.tensor(vrf_piraeus_pth_sd_cml['scaler'].scale_[:2])\n",
    "    )\n",
    ")\n",
    "sd_cml_model.load_state_dict(vrf_piraeus_pth_sd_cml['model_state_dict'])\n",
    "sd_cml_model.eval()\n",
    "\n",
    "\n",
    "# Instantiate FL VRF Model\n",
    "fl_model = ml.VesselRouteForecasting(\n",
    "    hidden_size=350, fc_layers=[150,], \n",
    "    scale=dict(\n",
    "        # Use the Same Statistics as the Piraeus Dataset Train Set (which is equal to the stats of SN-CML)\n",
    "        mu=torch.tensor(vrf_piraeus_pth_sn_cml['scaler'].mean_[:2]),    \n",
    "        sigma=torch.tensor(vrf_piraeus_pth_sn_cml['scaler'].scale_[:2])\n",
    "    )\n",
    ")\n",
    "fl_model.load_state_dict(fedvrf_piraeus_pth['model_state_dict'])\n",
    "fl_model.eval()\n",
    "\n",
    "# Instantiate PerFL VRF Model\n",
    "perfl_model = ml.VesselRouteForecasting(\n",
    "    hidden_size=350, fc_layers=[150,], \n",
    "    scale=dict(\n",
    "        # Use the Same Statistics as the Piraeus Dataset Train Set (which is equal to the stats of SN-CML)\n",
    "        mu=torch.tensor(vrf_piraeus_pth_sn_cml['scaler'].mean_[:2]),    \n",
    "        sigma=torch.tensor(vrf_piraeus_pth_sn_cml['scaler'].scale_[:2])\n",
    "    )\n",
    ")\n",
    "perfl_model.load_state_dict(perfl_fedvrf_piraeus_pth['model_state_dict'])\n",
    "perfl_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Get the date with maximal concurrent traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_flow = actual_points.groupby([actual_points.timestamp.dt.date, actual_points.timestamp.dt.hour]).apply(lambda l: l.vessel_id.nunique()).sort_index()\n",
    "traffic_flow.loc[traffic_flow == traffic_flow.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_traffic_slice = actual_points.loc[(actual_points.timestamp.dt.date == datetime.date(2019, 3, 28)) & (actual_points.timestamp.dt.hour == 5)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_traffic = max_traffic_slice.set_index('timestamp').resample(rule='15min').get_group('2019-03-28 05:15:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_traffic.vessel_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Area of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_area_grid(spatial_area, crs=4326, quadrat_width=1000):\n",
    "    '''\n",
    "        Segment a spatial area into (an equally spaced) square grid\n",
    "        \n",
    "        Input:\n",
    "            * spatial_area: The area to segment\n",
    "            * quadrat_width: The squares' width\n",
    "            \n",
    "        Output:\n",
    "            * A GeoSeries containing the grid's squares\n",
    "            \n",
    "        Note: the unit of the quadrat_width is in accord to the CRS of the spatial area.\n",
    "    '''\n",
    "    # quadrat_width is in the units the geometry is in, so we'll do a tenth of a degree\n",
    "    geometry_cut = stt.ox.utils_geo._quadrat_cut_geometry(spatial_area, quadrat_width=quadrat_width)\n",
    "    grid_gdf = gpd.GeoDataFrame(geometry_cut.geoms, columns=['geom'], geometry='geom', crs=crs)\n",
    "    return grid_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_area_proximity(points, geoms, predicate='contains', out_name='area_id'):\n",
    "    sindex = points.sindex\n",
    "    geoms_idx, points_idx = sindex.query_bulk(geoms.geometry, predicate=predicate)\n",
    "\n",
    "    points.loc[:, out_name] = np.nan\n",
    "    points.iloc[points_idx, points.columns.get_loc(out_name)] = geoms_idx\n",
    "    \n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_coverage = shapely.geometry.box(*[*[2.59e6, 4.53e6, 2.64e6, 4.59e6]])\n",
    "spatial_coverage_grid = create_area_grid(spatial_coverage, crs=3857, quadrat_width=1852) # Meters\n",
    "spatial_coverage_grid = spatial_coverage_grid.reset_index().rename({'index':'id'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Traffic Flow for Actual TimeFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_traffic_flow = classify_area_proximity(actual_traffic.copy(), spatial_coverage_grid.to_crs(4326)).groupby('area_id').apply(lambda l: l.vessel_id.nunique())\n",
    "# actual_traffic_flow = classify_area_proximity(actual_traffic.copy(), spatial_coverage_grid.to_crs(4326)).groupby('area_id').apply(lambda l: len(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpd.GeoDataFrame(\n",
    "    pd.merge(actual_traffic_flow.rename('traffic_flow'), spatial_coverage_grid, left_index=True, right_on='id'),\n",
    "    geometry='geom',\n",
    "    crs=3857\n",
    ").explore(column='traffic_flow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Predicted Traffic Flow for $\\Delta t = 15$ min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapely_coords_numpy = lambda l: np.array(*list(l.coords))\n",
    "\n",
    "def create_delta_dataset(segment, time_name, speed_name, course_name, crs=3857):\n",
    "    segment.sort_values(time_name, inplace=True)\n",
    "    \n",
    "    delta_curr = segment.to_crs(crs)[segment.geometry.name].apply(lambda l: pd.Series(shapely_coords_numpy(l), index=['dlon', 'dlat'])).diff()\n",
    "    delta_curr_feats = segment[[speed_name, course_name]].diff().rename({speed_name:'dspeed_curr', course_name:'dcourse_curr'}, axis=1)\n",
    "    delta_next = delta_curr.shift(-1)\n",
    "    delta_tau  = pd.merge(\n",
    "        segment[time_name].diff().rename('dt_curr'),\n",
    "        segment[time_name].diff().shift(-1).rename('dt_next'),\n",
    "        right_index=True, \n",
    "        left_index=True\n",
    "    )\n",
    "    \n",
    "    return delta_curr.join(delta_curr_feats).join(delta_tau).join(delta_next, lsuffix='_curr', rsuffix='_next').dropna(subset=['dt_curr', 'dt_next'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vrf_predict(traj, model, scaler, lookahead=900, step=30, crs=3857,\n",
    "                time_name='date_time_utc', speed_name='sog', course_name='cog',\n",
    "                feats_in=['dlon_curr', 'dlat_curr', 'dt_curr', 'dt_next']):\n",
    "    model.eval()\n",
    "    \n",
    "    traj_delta = create_delta_dataset(traj, time_name=time_name, speed_name=speed_name, course_name=course_name, crs=crs)\n",
    "    model_input = torch.tensor(traj_delta[feats_in].values)\n",
    "\n",
    "    for _ in range(step, lookahead+step+1, step):\n",
    "        model_input_sc = scaler.transform(model_input)\n",
    "\n",
    "        dxy_next = model(\n",
    "            torch.tensor(model_input_sc).unsqueeze(0).float(),\n",
    "            torch.tensor([len(model_input_sc)])\n",
    "        ).detach()\n",
    "\n",
    "        new_delta = torch.cat((dxy_next, torch.tensor([[step, step]])), dim=1)\n",
    "        model_input = torch.cat((model_input, new_delta), dim=0)\n",
    "\n",
    "    traj_start = np.array([\n",
    "        traj.iloc[[0]].to_crs(crs).geometry.x.values[0],\n",
    "        traj.iloc[[0]].to_crs(crs).geometry.y.values[0],\n",
    "        traj.iloc[[0]][time_name].values[0]\n",
    "    ])\n",
    "    \n",
    "    traj_pred = pd.DataFrame(\n",
    "        torch.cumsum(\n",
    "            torch.cat(\n",
    "                (torch.tensor(traj_start).unsqueeze(0), model_input[:, :3])\n",
    "            ),\n",
    "            dim=0\n",
    "        ).numpy(),\n",
    "        columns=['lon', 'lat', time_name]\n",
    "    )\n",
    "\n",
    "    traj_pred = gpd.GeoDataFrame(traj_pred, crs=crs, geometry=gpd.points_from_xy(traj_pred['lon'], traj_pred['lat'])).to_crs(4326)    \n",
    "    return traj_pred.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.tqdm.pandas()\n",
    "\n",
    "tff_res = dict()\n",
    "\n",
    "for name, model, scaler in zip(\n",
    "    ['sn_cml', 'sd_cml', 'fl', 'per_fl'],\n",
    "    [sn_cml_model, sd_cml_model, fl_model, perfl_model],\n",
    "    [vrf_piraeus_pth_sn_cml['scaler'], vrf_piraeus_pth_sd_cml['scaler'], vrf_piraeus_pth_sn_cml['scaler'], vrf_piraeus_pth_sn_cml['scaler']]\n",
    "):\n",
    "    # for each quarter of hour... (Use 15 min. trajectories for 30 min. forecasts)\n",
    "    max_traffic_slice_predictions = max_traffic_slice.set_index('timestamp').resample(rule='15min').apply(\n",
    "        # For each ```vessel_id``` use [Fed]VRF in order to infer its future trajectory, up to 30 min. with 30 sec. step (60 steps per forecast)\n",
    "        lambda sdf: sdf.groupby(['vessel_id']).progress_apply(\n",
    "            lambda vessel_id_traj: vrf_predict(\n",
    "                vessel_id_traj.sort_values('t'), \n",
    "                model=model, \n",
    "                scaler=scaler, \n",
    "                lookahead=1800, step=15, \n",
    "                time_name='t', speed_name='speed', course_name='course'\n",
    "            ).iloc[len(vessel_id_traj):].copy() if len(vessel_id_traj) >= 3 else None\n",
    "        )\n",
    "    ).reset_index(level=1, drop=False)\n",
    "    max_traffic_slice_predictions.t = pd.to_datetime(max_traffic_slice_predictions.t, unit='s')\n",
    "\n",
    "    tff_res[name] = classify_area_proximity(\n",
    "        # max_traffic_slice_predictions.query('t <= \"2019-03-28 05:30:00\"'), \n",
    "        max_traffic_slice_predictions.query('t < \"2019-03-28 05:30:00\" and t > \"2019-03-28 05:15:00\"').copy(), \n",
    "        spatial_coverage_grid.to_crs(4326)\n",
    "    ).groupby('area_id').apply(lambda l: l.vessel_id.nunique())\n",
    "    # ).groupby('area_id').apply(lambda l: len(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpd.GeoDataFrame(\n",
    "    pd.merge(tff_res['per_fl'].rename('traffic_flow'), spatial_coverage_grid, left_index=True, right_on='id'),\n",
    "    geometry='geom',\n",
    "    crs=3857\n",
    ").explore(column='traffic_flow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unifying to a Single Figure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat((actual_traffic_flow, tff_res['sn_cml'], tff_res['sd_cml'], tff_res['fl'], tff_res['per_fl']), axis=1).agg([min, max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax1 = gpd.GeoDataFrame(\n",
    "    pd.merge(actual_traffic_flow.rename('traffic_flow'), spatial_coverage_grid, left_index=True, right_on='id'),\n",
    "    geometry='geom',\n",
    "    crs=3857\n",
    ").plot(column='traffic_flow', cmap='YlOrRd', alpha=0.65)\n",
    "ctx.add_basemap(ax=ax1, source=ctx.providers.CartoDB.Positron, attribution='')\n",
    "ax1.axis('off')\n",
    "plt.savefig('tff_actual.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "ax2 = gpd.GeoDataFrame(\n",
    "    pd.merge(tff_res['sn_cml'].rename('traffic_flow'), spatial_coverage_grid, left_index=True, right_on='id'),\n",
    "    geometry='geom',\n",
    "    crs=3857\n",
    ").plot(column='traffic_flow', cmap='YlOrRd', alpha=0.65)\n",
    "ax2.set_xlim(*ax1.get_xlim()); ax2.set_ylim(*ax1.get_ylim()); ax2.axis('off')\n",
    "ctx.add_basemap(ax=ax2, source=ctx.providers.CartoDB.Positron, attribution='')\n",
    "plt.savefig('tff_sn_cml.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "ax3 = gpd.GeoDataFrame(\n",
    "    pd.merge(tff_res['sd_cml'].rename('traffic_flow'), spatial_coverage_grid, left_index=True, right_on='id'),\n",
    "    geometry='geom',\n",
    "    crs=3857\n",
    ").plot(column='traffic_flow', cmap='YlOrRd', alpha=0.65)\n",
    "ax3.set_xlim(*ax1.get_xlim()); ax3.set_ylim(*ax1.get_ylim()); ax3.axis('off')\n",
    "ctx.add_basemap(ax=ax3, source=ctx.providers.CartoDB.Positron, attribution='')\n",
    "plt.savefig('tff_sd_cml.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "ax4 = gpd.GeoDataFrame(\n",
    "    pd.merge(tff_res['fl'].rename('traffic_flow'), spatial_coverage_grid, left_index=True, right_on='id'),\n",
    "    geometry='geom',\n",
    "    crs=3857\n",
    ").plot(column='traffic_flow', cmap='YlOrRd', alpha=0.65)\n",
    "ax4.set_xlim(*ax1.get_xlim()); ax4.set_ylim(*ax1.get_ylim()); ax4.axis('off')\n",
    "ctx.add_basemap(ax=ax4, source=ctx.providers.CartoDB.Positron, attribution='')\n",
    "plt.savefig('tff_fl.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "ax5 = gpd.GeoDataFrame(\n",
    "    pd.merge(tff_res['per_fl'].rename('traffic_flow'), spatial_coverage_grid, left_index=True, right_on='id'),\n",
    "    geometry='geom',\n",
    "    crs=3857\n",
    ").plot(column='traffic_flow', cmap='YlOrRd', alpha=0.65)\n",
    "ax5.set_xlim(*ax1.get_xlim()); ax5.set_ylim(*ax1.get_ylim()); ax5.axis('off')\n",
    "ctx.add_basemap(ax=ax5, source=ctx.providers.CartoDB.Positron, attribution='')\n",
    "plt.savefig('tff_perfl.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    " \n",
    "# draw a new figure and replot the colorbar there\n",
    "fig, cax = plt.subplots(figsize=(7,3))\n",
    "sm = plt.cm.ScalarMappable(cmap='YlOrRd', norm=plt.Normalize(vmin=1, vmax=10))\n",
    "# sm = plt.cm.ScalarMappable(cmap='YlOrRd', norm=plt.Normalize(vmin=1, vmax=250))\n",
    "\n",
    "# empty array for the data range\n",
    "sm._A = []\n",
    "# add the colorbar to the figure\n",
    "cbar = fig.colorbar(sm, orientation='horizontal', shrink=0.94, pad=0.02, ax=cax)\n",
    "\n",
    "#     cax = plt.gcf().get_axes()[1]\n",
    "#and we can modify it, i.e.:\n",
    "cbar.ax.tick_params(labelsize=10)\n",
    "cbar.ax.set_xlabel('#Vessels', fontsize=10, labelpad=5)\n",
    "\n",
    "cax.remove()\n",
    "# cbar.ax.set_yticklabels([f'$10^{{\\, {np.int64(label)} }}$' if label.is_integer() else '' for label in cbar.ax.get_yticks()], rotation=90)  # horizontal colorbar\n",
    "cbar.ax.tick_params(labelsize=10)\n",
    "cbar.ax.xaxis.set_ticks_position(\"bottom\")\n",
    "plt.savefig('plot_onlycbar.png', dpi=300, bbox_inches='tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PyTorch)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
